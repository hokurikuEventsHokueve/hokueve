# scraperTEST.py
from bs4 import BeautifulSoup
from datetime import datetime
from supabase import create_client, Client
from playwright.sync_api import sync_playwright
import os

# Supabase èªè¨¼æƒ…å ±
url = os.environ.get("SUPABASE_URL")
key = os.environ.get("SUPABASE_ANON_KEY")
supabase: Client = create_client(url, key)

# æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
SEARCH_KEYWORD = "é‡‘æ²¢å¸‚"
BASE_URL = f"https://eplus.jp/sf/search?keyword={SEARCH_KEYWORD}"

def scrape_eplus():
    print("ğŸ” e+ï¼ˆPlaywrightï¼‰ã§ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—ä¸­...")
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(BASE_URL)
        # ãƒã‚±ãƒƒãƒˆè¦ç´ ãŒæç”»ã•ã‚Œã‚‹ã¾ã§æœ€å¤§10ç§’å¾…æ©Ÿ
        page.wait_for_selector("a.ticket-item", timeout=10000)
        html = page.content()
        browser.close()

    # BeautifulSoupã§HTMLã‚’è§£æ
    soup = BeautifulSoup(html, "html.parser")
    events = []

    cards = soup.select("a.ticket-item")
    for c in cards:
        title_elem = c.select_one(".ticket-item__title")
        year_elem = c.select_one(".ticket-item__yyyy")
        date_elem = c.select_one(".ticket-item__mmdd")
        venue_elem = c.select_one(".ticket-item__venue")
        link = c.get("href")

        title = title_elem.get_text(strip=True) if title_elem else "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜ï¼‰"
        year = year_elem.get_text(strip=True) if year_elem else ""
        mmdd = date_elem.get_text(strip=True) if date_elem else ""
        venue = venue_elem.get_text(strip=True) if venue_elem else ""
        event_url = f"https://eplus.jp{link}" if link and link.startswith("/") else link

        # æ—¥ä»˜ã‚’æ•´å½¢
        try:
            date_str = f"{year}.{mmdd}".replace("/", ".")
            date = datetime.strptime(date_str, "%Y.%m.%d").date()
        except Exception:
            date = None

        events.append({
            "title": title,
            "date": date,
            "venue": venue,
            "url": event_url,
            "image_url": None,
            "source": "eplus",
            "created_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat(),
        })

    print(f"âœ… {len(events)} ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ")
    return events

def save_to_supabase(events):
    if not events:
        print("âš ï¸ ç™»éŒ²ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    try:
        supabase.table("events").insert(events).execute()
        print(f"ğŸ’¾ Supabaseã« {len(events)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ Supabaseã¸ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    events = scrape_eplus()
    save_to_supabase(events)
